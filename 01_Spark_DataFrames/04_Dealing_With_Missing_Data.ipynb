{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Spark session.\n",
    "spark = SparkSession.builder.appName('miss').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can infer the schema/types (only in CSV), and header tells us\n",
    "# that the first row are the names of the columns.\n",
    "df = spark.read.csv('Data/contains_null.csv', \n",
    "                    header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We have 3 columns, in Name and Sales columns there's missing values.\n",
    "# Some rows are missing one or the other, and some both.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This drops any rows that contains any row with missing data.\n",
    "# df.na.drop().show()\n",
    "# If you specify a threshold value, it will drop rows that have\n",
    "# less than this certain number of non-null values.\n",
    "# 0 - Drops 0 - none has less than 0 non-null.\n",
    "# 1 - Drops 0 - all columns have at least one value defined.\n",
    "# 2 - Drops 1 - one column has only 1 non-null from 3 total. \n",
    "# 3 - Drops 3 - leaves only the one with all 3 columns defined.\n",
    "# 4 - Drops all - with 3 columns its impossible to have 4 non-null.\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can also 'how' to drop either doing it if 'any'value is null,\n",
    "# or if 'all' of them are. No row has all nulls so no row is dropped.\n",
    "# The 'how' parameter is overwritten by 'thresh' so be careful.\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another useful parameter is 'subset' which allows to specify from\n",
    "# which columns you want to look for missing data for dropping.\n",
    "# Sometimes some columns may be tolerable to have missing data but\n",
    "# not others, like 'Id' or 'Sales'.\n",
    "df.na.drop(subset=['Sales', 'Id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema so you can see later how Spark can automatically\n",
    "# fill certain columns according to the type of fill value given.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----+\n",
      "|  Id|          Name|Sales|\n",
      "+----+--------------+-----+\n",
      "|emp1|          John| null|\n",
      "|emp2|FILL VALUE STR| null|\n",
      "|emp3|FILL VALUE STR|345.0|\n",
      "|emp4|         Cindy|456.0|\n",
      "+----+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From schema, Spark automatically knows to fill in string values\n",
    "# if you provide it with a string, same with numeric values.\n",
    "df.na.fill('FILL VALUE STR').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| -1.0|\n",
      "|emp2|No Name| -1.0|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The inner call returns a DataFrame on which you can in turn call\n",
    "# again the fill method to keep filling data. You can only fill one\n",
    "# at a time with a call but if you provide a subset of Columns, it\n",
    "# will target those columns specifically (in case you don't want to\n",
    "# target all string columns with null values in a single fill).\n",
    "(df.na.fill('No Name', subset=['Name'])).na.fill(-1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some people fill numerical values with the mean value. Use the mean\n",
    "# function on a given column, and collect to put all results into a\n",
    "# list which we can iterate over.\n",
    "mean_vals = df.select(f.mean(df['Sales'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember collect gives a list of rows, and then each row is a Row\n",
    "# object from which you access elements in order.\n",
    "mean_sales = mean_vals[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(mean_sales, ['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
